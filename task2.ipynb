{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6bR3CBsM_l4",
        "outputId": "2653fc69-f917-442d-c7f0-f203a392742e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear: RMSE=0.787, R2=0.200, Time=0.045s, Non-zero Coeffs=77\n",
            "SGD: RMSE=79803027662.477, R2=-8223039543245029769216.000, Time=0.012s, Non-zero Coeffs=77\n",
            "Ridge: RMSE=0.787, R2=0.200, Time=0.012s, Non-zero Coeffs=77\n",
            "Lasso: RMSE=0.769, R2=0.236, Time=0.008s, Non-zero Coeffs=13\n",
            "ElasticNet: RMSE=0.780, R2=0.215, Time=0.012s, Non-zero Coeffs=19\n",
            "Best Ridge alpha: {'alpha': 10}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ],
      "source": [
        "# Task 02: Model Training Fundamentals with Wine Quality Dataset\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import time\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('winequality-white.csv', sep=';')\n",
        "X = data.drop('quality', axis=1)\n",
        "y = data['quality']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train_scaled)\n",
        "X_test_poly = poly.transform(X_test_scaled)\n",
        "\n",
        "# Train and evaluate models\n",
        "models = {\n",
        "    'Linear': LinearRegression(),\n",
        "    'SGD': SGDRegressor(penalty=None, random_state=42),\n",
        "    'Ridge': Ridge(alpha=1.0),\n",
        "    'Lasso': Lasso(alpha=0.1),\n",
        "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "}\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    non_zero_coeffs = np.sum(np.abs(model.coef_) > 1e-10) if hasattr(model, 'coef_') else len(model.coef_)\n",
        "    results.append({'Model': name, 'RMSE': rmse, 'R2': r2, 'Training Time': train_time, 'Non-zero Coefficients': non_zero_coeffs})\n",
        "    print(f\"{name}: RMSE={rmse:.3f}, R2={r2:.3f}, Time={train_time:.3f}s, Non-zero Coeffs={non_zero_coeffs}\")\n",
        "\n",
        "# Hyperparameter tuning (Ridge example)\n",
        "param_grid = {'alpha': [0.01, 0.1, 1, 10]}\n",
        "grid_search = GridSearchCV(Ridge(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "print(\"Best Ridge alpha:\", grid_search.best_params_)\n",
        "\n",
        "# Plot learning curves\n",
        "def plot_learning_curves(model, X, y, name):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    train_errors, val_errors = [], []\n",
        "    sizes = range(1, len(X_train), 100)\n",
        "    for m in sizes:\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_pred = model.predict(X_train[:m])\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        train_errors.append(np.sqrt(mean_squared_error(y_train[:m], y_train_pred)))\n",
        "        val_errors.append(np.sqrt(mean_squared_error(y_val, y_val_pred)))\n",
        "    plt.plot(sizes, train_errors, 'r-+', label='Training RMSE')\n",
        "    plt.plot(sizes, val_errors, 'b-', label='Validation RMSE')\n",
        "    plt.xlabel('Training Set Size')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title(f'Learning Curves: {name}')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'learning_curve_{name}.png')\n",
        "    plt.close()\n",
        "\n",
        "for name, model in models.items():\n",
        "    plot_learning_curves(model, X_train_poly, y_train, name)\n",
        "\n",
        "# Coefficient analysis plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "for name, model in models.items():\n",
        "    if hasattr(model, 'coef_'):\n",
        "        plt.plot(np.abs(model.coef_), label=name)\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('Coefficient Magnitude')\n",
        "plt.title('Coefficient Magnitudes Across Models')\n",
        "plt.legend()\n",
        "plt.savefig('coefficient_plot.png')\n",
        "plt.close()\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('model_comparison.csv', index=False)"
      ]
    }
  ]
}